{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from skimage.measure import block_reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import deep_utils as du\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3d8d187ad609>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/caiomoraes/miniconda3/envs/corning_inspect/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/caiomoraes/miniconda3/envs/corning_inspect/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/caiomoraes/miniconda3/envs/corning_inspect/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/caiomoraes/miniconda3/envs/corning_inspect/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/caiomoraes/miniconda3/envs/corning_inspect/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "train_labels (55000, 10)\n",
      "train_dataset (55000, 28, 28, 1)\n",
      "validation_labels (5000, 10)\n",
      "validation_dataset (5000, 28, 28, 1)\n",
      "test_labels (10000, 10)\n",
      "test_dataset (10000, 28, 28, 1) \n",
      "\n",
      "train_dataset (55000, 32, 32, 1)\n",
      "validation_dataset (5000, 32, 32, 1)\n",
      "test_dataset (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True, reshape=False)\n",
    "\n",
    "train_dataset = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "validation_dataset = mnist.validation.images\n",
    "validation_labels = mnist.validation.labels\n",
    "test_dataset = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "print('train_labels', train_labels.shape)\n",
    "print('train_dataset', train_dataset.shape)\n",
    "print('validation_labels', validation_labels.shape)\n",
    "print('validation_dataset', validation_dataset.shape)\n",
    "print('test_labels', test_labels.shape)\n",
    "print('test_dataset', test_dataset.shape, '\\n')\n",
    "\n",
    "train_dataset = np.pad(train_dataset, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "validation_dataset = np.pad(validation_dataset, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "test_dataset = np.pad(test_dataset, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')\n",
    "\n",
    "print('train_dataset', train_dataset.shape)\n",
    "print('validation_dataset', validation_dataset.shape)\n",
    "print('test_dataset', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_he(np_ar, previous_neurons_num):\n",
    "    return np.sqrt(2 / previous_neurons_num) * np.random.randn(*np_ar.shape)\n",
    "\n",
    "\n",
    "layers = []\n",
    "\n",
    "conv1_w = weight_init_he(np.zeros((5, 5, 1, 6)), previous_neurons_num=32*32)\n",
    "conv1_b = 0.01\n",
    "layers.append(conv1_w)\n",
    "\n",
    "conv2_w = weight_init_he(np.zeros((5, 5, layers[-1].shape[-1], 16)), previous_neurons_num=6*14*14)\n",
    "conv2_b = 0.01\n",
    "layers.append(conv2_w)\n",
    "\n",
    "conv3_w = weight_init_he(np.zeros((5, 5, layers[-1].shape[-1], 10)), previous_neurons_num=16*5*5)\n",
    "conv3_b = 0.01\n",
    "layers.append(conv3_w)\n",
    "\n",
    "dens1_w = weight_init_he(np.zeros((layers[-1].shape[-1], 10)), previous_neurons_num=120)\n",
    "dens1_b = 0.01\n",
    "layers.append(dens1_w)\n",
    "\n",
    "# dens2_w = weight_init_he(np.zeros((layers[-1].shape[-1], 10)), previous_neurons_num=84)\n",
    "# dens2_b = 0.01\n",
    "# layers.append(dens2_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_he(np_ar, previous_neurons_num):\n",
    "    return np.sqrt(2 / previous_neurons_num) * np.random.randn(*np_ar.shape)\n",
    "\n",
    "def pooling(np_ar, pooling_func, stride=2):\n",
    "    # block size for both single image or images array\n",
    "    block_size = *((len(np_ar.shape)-3)*[1]), stride, stride, 1\n",
    "    return block_reduce(np_ar, block_size=(block_size), func=pooling_func)\n",
    "\n",
    "def pipe_operation_conv(np_ar, conv_w, conv_b, activ_func=du.relu):\n",
    "    \n",
    "    hks = int((conv_w.shape[1] - 1) / 2)\n",
    "    conv = np.zeros((np_ar.shape[0] - 2*hks, np_ar.shape[1] - 2*hks, conv_w.shape[-1]))\n",
    "    \n",
    "    for i in range(conv_w.shape[-1]):\n",
    "        conv[:, :, i] = activ_func(du.convolution(np_ar, conv_w[:, :, :, i], stride=1, bias=conv_b))\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def pipe_operation_dense(np_ar, den_w, den_b):\n",
    "    return np.dot(np_ar, den_w) + den_b\n",
    "\n",
    "def forward(img):\n",
    "    c1 = pipe_operation_conv(img, conv1_w, conv1_b)\n",
    "    p1 = pooling(c1, np.mean, stride=2)\n",
    "\n",
    "    c2 = pipe_operation_conv(p1, conv2_w, conv2_b)\n",
    "    p2 = pooling(c2, np.mean, stride=2)\n",
    "\n",
    "    c3 = pipe_operation_conv(p2, conv3_w, conv3_b)\n",
    "    c3 = c3.squeeze()\n",
    "\n",
    "    d1 = pipe_operation_dense(c3, dens1_w, dens1_b)\n",
    "#     d2 = pipe_operation_dense(d1, dens2_w, dens2_b)\n",
    "    \n",
    "    return d1\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def batch_gen(dataset, dataset_labels, batch_size, steps, normalization=True):\n",
    "    for batch in range(steps):\n",
    "#         print('batch', batch)\n",
    "        batch_data    = np.take(dataset       , range(32*batch, 32*(batch+1)), axis=0, mode='wrap')\n",
    "        batch_classes = np.take(dataset_labels, range(32*batch, 32*(batch+1)), axis=0, mode='wrap')\n",
    "        if normalization:\n",
    "            batch_mean = batch_data.sum(axis=0) / len(batch_data)\n",
    "            delta = batch_data - batch_mean\n",
    "            batch_var = ((delta) ** 2).sum(axis=0) / len(batch_data)\n",
    "            resulting_batch = (delta)/ ( (batch_var + 1e-8) ** 0.5 )\n",
    "        else:\n",
    "            resulting_batch = batch_data\n",
    "\n",
    "#         plt.imshow(np.squeeze(resulting_batch[0,:,:,:]),cmap='gray')\n",
    "#         plt.show()\n",
    "\n",
    "#         plt.imshow(np.squeeze(resulting_batch[4,:,:,:]),cmap='gray')\n",
    "#         plt.show()\n",
    "\n",
    "#         print('\\n=================================')\n",
    "#         print('============== Case 3 Implementation ===================')\n",
    "#         print(\"Data Shape: \",resulting_batch.shape)\n",
    "#         print(\"Data Max: \",resulting_batch.max())\n",
    "#         print(\"Data Min: \",resulting_batch.min())\n",
    "#         print(\"Data Mean: \",resulting_batch.mean())\n",
    "#         print(\"Data Variance: \",resulting_batch.var())\n",
    "#         plt.hist(resulting_batch.flatten() ,bins='auto')\n",
    "#         plt.show()\n",
    "#         print('=================================')            \n",
    "        \n",
    "        yield resulting_batch, batch_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 6)\n",
      "(5, 5, 6, 16)\n",
      "(5, 5, 16, 10)\n",
      "(10, 10)\n",
      "144000000000\n",
      "9.841899722852105e-07\n"
     ]
    }
   ],
   "source": [
    "total = 1\n",
    "for layer in layers:\n",
    "    print(layer.shape)\n",
    "    for dim in layer.shape:\n",
    "        total *= dim\n",
    "print(total)\n",
    "print(total/146313216000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset[0].shape (32, 32, 1)\n",
      "CONV\n",
      "c1.shape (28, 28, 6)\n",
      "p1.shape (14, 14, 6)\n",
      "c2.shape (10, 10, 16)\n",
      "p2.shape (5, 5, 16)\n",
      "c3.shape (1, 1, 10)\n",
      "c3.shape (10,)\n",
      "DENSE\n",
      "d1.shape (10,)\n"
     ]
    }
   ],
   "source": [
    "print('train_dataset[0].shape', train_dataset[0].shape)\n",
    "\n",
    "print('CONV')\n",
    "c1 = pipe_operation_conv(train_dataset[0], conv1_w, conv1_b)\n",
    "print('c1.shape', c1.shape)\n",
    "p1 = pooling(c1, np.mean, stride=2)\n",
    "print('p1.shape', p1.shape)\n",
    "\n",
    "c2 = pipe_operation_conv(p1, conv2_w, conv2_b)\n",
    "print('c2.shape', c2.shape)\n",
    "p2 = pooling(c2, np.mean, stride=2)\n",
    "print('p2.shape', p2.shape)\n",
    "\n",
    "c3 = pipe_operation_conv(p2, conv3_w, conv3_b)\n",
    "print('c3.shape', c3.shape)\n",
    "c3 = c3.squeeze()\n",
    "print('c3.shape', c3.shape)\n",
    "\n",
    "print('DENSE')\n",
    "d1 = np.dot(c3, dens1_w) + dens1_b\n",
    "print('d1.shape', d1.shape)\n",
    "\n",
    "# d2 = np.dot(d1, dens2_w) + dens2_b\n",
    "# print('d2.shape', d2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH:  1\n",
      "layer: 0\n",
      "(0, 0, 0, 0)\tLoss:\t 2.3002267500592133\n",
      "(0, 0, 0, 1)\tLoss:\t 2.30020275965089\n",
      "(0, 0, 0, 2)\tLoss:\t 2.300200130039215\n",
      "(0, 0, 0, 3)\tLoss:\t 2.3001656424895742\n",
      "(0, 0, 0, 4)\tLoss:\t 2.3001289280464783\n",
      "(0, 0, 0, 5)\tLoss:\t 2.300123907413152\n",
      "(0, 1, 0, 0)\tLoss:\t 2.3000781191681785\n",
      "(0, 1, 0, 1)\tLoss:\t 2.300021665002039\n",
      "(0, 1, 0, 2)\tLoss:\t 2.3000179213155088\n",
      "(0, 1, 0, 3)\tLoss:\t 2.299902500244708\n",
      "(0, 1, 0, 4)\tLoss:\t 2.2999025140593057\n",
      "(0, 1, 0, 5)\tLoss:\t 2.2998975177469765\n",
      "(0, 2, 0, 0)\tLoss:\t 2.2998561887218383\n",
      "(0, 2, 0, 1)\tLoss:\t 2.299700439463413\n",
      "(0, 2, 0, 2)\tLoss:\t 2.2996915269255696\n",
      "(0, 2, 0, 3)\tLoss:\t 2.2995367914540754\n",
      "(0, 2, 0, 4)\tLoss:\t 2.299515982818355\n",
      "(0, 2, 0, 5)\tLoss:\t 2.2995177050166484\n",
      "(0, 3, 0, 0)\tLoss:\t 2.2994997167421545\n",
      "(0, 3, 0, 1)\tLoss:\t 2.2993355923705665\n",
      "(0, 3, 0, 2)\tLoss:\t 2.299334782933667\n",
      "(0, 3, 0, 3)\tLoss:\t 2.2991409527098083\n",
      "(0, 3, 0, 4)\tLoss:\t 2.299116135518292\n",
      "(0, 3, 0, 5)\tLoss:\t 2.299114590612799\n",
      "(0, 4, 0, 0)\tLoss:\t 2.299102908225802\n",
      "(0, 4, 0, 1)\tLoss:\t 2.298989256402704\n",
      "(0, 4, 0, 2)\tLoss:\t 2.298989310154583\n",
      "(0, 4, 0, 3)\tLoss:\t 2.2988318387021254\n",
      "(0, 4, 0, 4)\tLoss:\t 2.298802792091265\n",
      "(0, 4, 0, 5)\tLoss:\t 2.298787370792803\n",
      "(1, 0, 0, 0)\tLoss:\t 2.298776248049091\n",
      "(1, 0, 0, 1)\tLoss:\t 2.298760558263904\n",
      "(1, 0, 0, 2)\tLoss:\t 2.298731003804555\n",
      "(1, 0, 0, 3)\tLoss:\t 2.298731290873136\n",
      "(1, 0, 0, 4)\tLoss:\t 2.298724764545951\n",
      "(1, 0, 0, 5)\tLoss:\t 2.298725643121614\n",
      "(1, 1, 0, 0)\tLoss:\t 2.2986874980666068\n",
      "(1, 1, 0, 1)\tLoss:\t 2.2986575855850995\n",
      "(1, 1, 0, 2)\tLoss:\t 2.2986198929147923\n",
      "(1, 1, 0, 3)\tLoss:\t 2.298619883369395\n",
      "(1, 1, 0, 4)\tLoss:\t 2.298617879688744\n",
      "(1, 1, 0, 5)\tLoss:\t 2.298607529184358\n",
      "(1, 2, 0, 0)\tLoss:\t 2.29858181813249\n",
      "(1, 2, 0, 1)\tLoss:\t 2.298532071264697\n",
      "(1, 2, 0, 2)\tLoss:\t 2.298500978332269\n",
      "(1, 2, 0, 3)\tLoss:\t 2.298499781578971\n",
      "(1, 2, 0, 4)\tLoss:\t 2.2984711495244388\n",
      "(1, 2, 0, 5)\tLoss:\t 2.298470147746718\n",
      "(1, 3, 0, 0)\tLoss:\t 2.2984594908211333\n",
      "(1, 3, 0, 1)\tLoss:\t 2.2983870324792504\n",
      "(1, 3, 0, 2)\tLoss:\t 2.298369459789751\n",
      "(1, 3, 0, 3)\tLoss:\t 2.2983417510266277\n",
      "(1, 3, 0, 4)\tLoss:\t 2.298316515887266\n",
      "(1, 3, 0, 5)\tLoss:\t 2.2983032639989416\n",
      "(1, 4, 0, 0)\tLoss:\t 2.298293056731814\n",
      "(1, 4, 0, 1)\tLoss:\t 2.2982426285696542\n",
      "(1, 4, 0, 2)\tLoss:\t 2.298239383299168\n",
      "(1, 4, 0, 3)\tLoss:\t 2.2981738205221895\n",
      "(1, 4, 0, 4)\tLoss:\t 2.298146970966121\n",
      "(1, 4, 0, 5)\tLoss:\t 2.29811013985747\n",
      "(2, 0, 0, 0)\tLoss:\t 2.2981050560218748\n",
      "(2, 0, 0, 1)\tLoss:\t 2.298104342141423\n",
      "(2, 0, 0, 2)\tLoss:\t 2.297979353370862\n",
      "(2, 0, 0, 3)\tLoss:\t 2.297925670432028\n",
      "(2, 0, 0, 4)\tLoss:\t 2.297927613068823\n",
      "(2, 0, 0, 5)\tLoss:\t 2.297903050714646\n",
      "(2, 1, 0, 0)\tLoss:\t 2.2978967295459176\n",
      "(2, 1, 0, 1)\tLoss:\t 2.2978941317875226\n",
      "(2, 1, 0, 2)\tLoss:\t 2.297777454567062\n",
      "(2, 1, 0, 3)\tLoss:\t 2.2977519467034977\n",
      "(2, 1, 0, 4)\tLoss:\t 2.2977521333351247\n",
      "(2, 1, 0, 5)\tLoss:\t 2.297755136750803\n",
      "(2, 2, 0, 0)\tLoss:\t 2.2977550360269223\n",
      "(2, 2, 0, 1)\tLoss:\t 2.297748568523809\n",
      "(2, 2, 0, 2)\tLoss:\t 2.2976695538279444\n",
      "(2, 2, 0, 3)\tLoss:\t 2.2976583867588394\n",
      "(2, 2, 0, 4)\tLoss:\t 2.2976318594849827\n",
      "(2, 2, 0, 5)\tLoss:\t 2.2976321013426118\n",
      "(2, 3, 0, 0)\tLoss:\t 2.297631503854773\n",
      "(2, 3, 0, 1)\tLoss:\t 2.297627766072821\n",
      "(2, 3, 0, 2)\tLoss:\t 2.297593544526708\n",
      "(2, 3, 0, 3)\tLoss:\t 2.297593543476346\n",
      "(2, 3, 0, 4)\tLoss:\t 2.297585478869216\n",
      "(2, 3, 0, 5)\tLoss:\t 2.2975750121519827\n",
      "(2, 4, 0, 0)\tLoss:\t 2.2975698365145254\n",
      "(2, 4, 0, 1)\tLoss:\t 2.2975691450050886\n",
      "(2, 4, 0, 2)\tLoss:\t 2.297560001573667\n",
      "(2, 4, 0, 3)\tLoss:\t 2.2975486352824595\n",
      "(2, 4, 0, 4)\tLoss:\t 2.297544479232953\n",
      "(2, 4, 0, 5)\tLoss:\t 2.2975262332199646\n",
      "(3, 0, 0, 0)\tLoss:\t 2.2975264428055318\n",
      "(3, 0, 0, 1)\tLoss:\t 2.297515678086667\n",
      "(3, 0, 0, 2)\tLoss:\t 2.297428791429966\n",
      "(3, 0, 0, 3)\tLoss:\t 2.297337806061846\n",
      "(3, 0, 0, 4)\tLoss:\t 2.29733783096517\n",
      "(3, 0, 0, 5)\tLoss:\t 2.2973265101966014\n",
      "(3, 1, 0, 0)\tLoss:\t 2.2973263412491023\n",
      "(3, 1, 0, 1)\tLoss:\t 2.297314400301582\n",
      "(3, 1, 0, 2)\tLoss:\t 2.2972397265817834\n",
      "(3, 1, 0, 3)\tLoss:\t 2.2972080270499307\n",
      "(3, 1, 0, 4)\tLoss:\t 2.297192907992402\n",
      "(3, 1, 0, 5)\tLoss:\t 2.29717111758685\n",
      "(3, 2, 0, 0)\tLoss:\t 2.2971631485357564\n",
      "(3, 2, 0, 1)\tLoss:\t 2.2971577507062158\n",
      "(3, 2, 0, 2)\tLoss:\t 2.2970800799432585\n",
      "(3, 2, 0, 3)\tLoss:\t 2.297054224496897\n",
      "(3, 2, 0, 4)\tLoss:\t 2.297041219982465\n",
      "(3, 2, 0, 5)\tLoss:\t 2.2970393383180028\n",
      "(3, 3, 0, 0)\tLoss:\t 2.297014202363159\n",
      "(3, 3, 0, 1)\tLoss:\t 2.297011111072988\n",
      "(3, 3, 0, 2)\tLoss:\t 2.2969889834964223\n",
      "(3, 3, 0, 3)\tLoss:\t 2.2969778651456108\n",
      "(3, 3, 0, 4)\tLoss:\t 2.296969610595448\n",
      "(3, 3, 0, 5)\tLoss:\t 2.296945032394701\n",
      "(3, 4, 0, 0)\tLoss:\t 2.296915382564969\n",
      "(3, 4, 0, 1)\tLoss:\t 2.296915132280235\n",
      "(3, 4, 0, 2)\tLoss:\t 2.296902137563609\n",
      "(3, 4, 0, 3)\tLoss:\t 2.2969019786990468\n",
      "(3, 4, 0, 4)\tLoss:\t 2.296902100666571\n",
      "(3, 4, 0, 5)\tLoss:\t 2.2968700014742693\n",
      "(4, 0, 0, 0)\tLoss:\t 2.296852071453822\n",
      "(4, 0, 0, 1)\tLoss:\t 2.296841181405962\n",
      "(4, 0, 0, 2)\tLoss:\t 2.2967284678636437\n",
      "(4, 0, 0, 3)\tLoss:\t 2.2966851409909887\n",
      "(4, 0, 0, 4)\tLoss:\t 2.296679323527384\n",
      "(4, 0, 0, 5)\tLoss:\t 2.296673068950736\n",
      "(4, 1, 0, 0)\tLoss:\t 2.2966698461256136\n",
      "(4, 1, 0, 1)\tLoss:\t 2.2966658268617772\n",
      "(4, 1, 0, 2)\tLoss:\t 2.2965732347891477\n",
      "(4, 1, 0, 3)\tLoss:\t 2.296557589150419\n",
      "(4, 1, 0, 4)\tLoss:\t 2.296552261818461\n",
      "(4, 1, 0, 5)\tLoss:\t 2.296551277048728\n",
      "(4, 2, 0, 0)\tLoss:\t 2.296525709642556\n",
      "(4, 2, 0, 1)\tLoss:\t 2.296513223330802\n",
      "(4, 2, 0, 2)\tLoss:\t 2.2964753162805915\n",
      "(4, 2, 0, 3)\tLoss:\t 2.2964531169023323\n",
      "(4, 2, 0, 4)\tLoss:\t 2.296450623242777\n",
      "(4, 2, 0, 5)\tLoss:\t 2.296420730822592\n",
      "(4, 3, 0, 0)\tLoss:\t 2.2963585946415597\n",
      "(4, 3, 0, 1)\tLoss:\t 2.296349985892445\n",
      "(4, 3, 0, 2)\tLoss:\t 2.2963375537347552\n",
      "(4, 3, 0, 3)\tLoss:\t 2.2963255199270227\n",
      "(4, 3, 0, 4)\tLoss:\t 2.296320726585025\n",
      "(4, 3, 0, 5)\tLoss:\t 2.296289905152452\n",
      "(4, 4, 0, 0)\tLoss:\t 2.2962394600070777\n",
      "(4, 4, 0, 1)\tLoss:\t 2.2962402622017115\n",
      "(4, 4, 0, 2)\tLoss:\t 2.296220486822606\n",
      "(4, 4, 0, 3)\tLoss:\t 2.2962207601257543\n",
      "(4, 4, 0, 4)\tLoss:\t 2.296213239569293\n",
      "(4, 4, 0, 5)\tLoss:\t 2.2961966921706067\n",
      "layer: 1\n",
      "(0, 0, 0, 0)\tLoss:\t 2.2961800940112913\n",
      "(0, 0, 0, 1)\tLoss:\t 2.296179580352198\n",
      "(0, 0, 0, 2)\tLoss:\t 2.296164222262284\n",
      "(0, 0, 0, 3)\tLoss:\t 2.296164213053677\n",
      "(0, 0, 0, 4)\tLoss:\t 2.2961521833810576\n",
      "(0, 0, 0, 5)\tLoss:\t 2.2961521443790094\n",
      "(0, 0, 0, 6)\tLoss:\t 2.2961497562711166\n",
      "(0, 0, 0, 7)\tLoss:\t 2.296149732775972\n",
      "(0, 0, 0, 8)\tLoss:\t 2.2961489500336025\n",
      "(0, 0, 0, 9)\tLoss:\t 2.296148880353843\n",
      "(0, 0, 0, 10)\tLoss:\t 2.2961480510527004\n",
      "(0, 0, 0, 11)\tLoss:\t 2.2961477789759472\n",
      "(0, 0, 0, 12)\tLoss:\t 2.296147497341109\n",
      "(0, 0, 0, 13)\tLoss:\t 2.296144227575202\n",
      "(0, 0, 0, 14)\tLoss:\t 2.296144013852767\n",
      "(0, 0, 0, 15)\tLoss:\t 2.2961435125148064\n",
      "(0, 0, 1, 0)\tLoss:\t 2.296138141130832\n",
      "(0, 0, 1, 1)\tLoss:\t 2.296137781975946\n",
      "(0, 0, 1, 2)\tLoss:\t 2.2961376485539953\n",
      "(0, 0, 1, 3)\tLoss:\t 2.296135009866134\n",
      "(0, 0, 1, 4)\tLoss:\t 2.2961349275053107\n",
      "(0, 0, 1, 5)\tLoss:\t 2.2961349272040943\n",
      "(0, 0, 1, 6)\tLoss:\t 2.2961161879569243\n",
      "(0, 0, 1, 7)\tLoss:\t 2.2961161289707217\n",
      "(0, 0, 1, 8)\tLoss:\t 2.2961093264650305\n",
      "(0, 0, 1, 9)\tLoss:\t 2.2961091313050535\n",
      "(0, 0, 1, 10)\tLoss:\t 2.2961091266086724\n",
      "(0, 0, 1, 11)\tLoss:\t 2.2961090031493985\n",
      "(0, 0, 1, 12)\tLoss:\t 2.296109002857548\n",
      "(0, 0, 1, 13)\tLoss:\t 2.2961088621879533\n",
      "(0, 0, 1, 14)\tLoss:\t 2.2961088205058946\n",
      "(0, 0, 1, 15)\tLoss:\t 2.2961083863005896\n",
      "(0, 0, 2, 0)\tLoss:\t 2.2961073652159003\n",
      "(0, 0, 2, 1)\tLoss:\t 2.296107340947117\n",
      "(0, 0, 2, 2)\tLoss:\t 2.2961034919292316\n",
      "(0, 0, 2, 3)\tLoss:\t 2.296100854356987\n",
      "(0, 0, 2, 4)\tLoss:\t 2.2960958376681724\n",
      "(0, 0, 2, 5)\tLoss:\t 2.296095834684009\n",
      "(0, 0, 2, 6)\tLoss:\t 2.296095830131149\n",
      "(0, 0, 2, 7)\tLoss:\t 2.296095583196243\n",
      "(0, 0, 2, 8)\tLoss:\t 2.2960955815741384\n",
      "(0, 0, 2, 9)\tLoss:\t 2.29609552649186\n",
      "(0, 0, 2, 10)\tLoss:\t 2.2960950435890757\n",
      "(0, 0, 2, 11)\tLoss:\t 2.2960946551886856\n",
      "(0, 0, 2, 12)\tLoss:\t 2.296094427473543\n",
      "(0, 0, 2, 13)\tLoss:\t 2.2960915410280194\n",
      "(0, 0, 2, 14)\tLoss:\t 2.296091515349616\n",
      "(0, 0, 2, 15)\tLoss:\t 2.2960915150768133\n",
      "(0, 0, 3, 0)\tLoss:\t 2.2960900133968014\n",
      "(0, 0, 3, 1)\tLoss:\t 2.296089218054817\n",
      "(0, 0, 3, 2)\tLoss:\t 2.296077076430225\n",
      "(0, 0, 3, 3)\tLoss:\t 2.2960673776950893\n",
      "(0, 0, 3, 4)\tLoss:\t 2.296065168015078\n",
      "(0, 0, 3, 5)\tLoss:\t 2.2960651144677597\n",
      "(0, 0, 3, 6)\tLoss:\t 2.2960519049769976\n",
      "(0, 0, 3, 7)\tLoss:\t 2.296046095771578\n",
      "(0, 0, 3, 8)\tLoss:\t 2.2960460614635196\n",
      "(0, 0, 3, 9)\tLoss:\t 2.296045184163291\n",
      "(0, 0, 3, 10)\tLoss:\t 2.29604279841276\n",
      "(0, 0, 3, 11)\tLoss:\t 2.296038720352664\n",
      "(0, 0, 3, 12)\tLoss:\t 2.296037452432751\n",
      "(0, 0, 3, 13)\tLoss:\t 2.2960357501250237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 3, 14)\tLoss:\t 2.296035236840458\n",
      "(0, 0, 3, 15)\tLoss:\t 2.2960349900668553\n",
      "(0, 0, 4, 0)\tLoss:\t 2.296029152660278\n",
      "(0, 0, 4, 1)\tLoss:\t 2.2960285526540485\n",
      "(0, 0, 4, 2)\tLoss:\t 2.296019982200151\n",
      "(0, 0, 4, 3)\tLoss:\t 2.296015568122077\n",
      "(0, 0, 4, 4)\tLoss:\t 2.2960117211665523\n",
      "(0, 0, 4, 5)\tLoss:\t 2.296011711674212\n",
      "(0, 0, 4, 6)\tLoss:\t 2.2959996261050915\n",
      "(0, 0, 4, 7)\tLoss:\t 2.2959962638670515\n",
      "(0, 0, 4, 8)\tLoss:\t 2.2959941946337112\n",
      "(0, 0, 4, 9)\tLoss:\t 2.2959939206110276\n",
      "(0, 0, 4, 10)\tLoss:\t 2.2959938584406245\n",
      "(0, 0, 4, 11)\tLoss:\t 2.2959905427730964\n",
      "(0, 0, 4, 12)\tLoss:\t 2.295989113905251\n",
      "(0, 0, 4, 13)\tLoss:\t 2.295988940170379\n",
      "(0, 0, 4, 14)\tLoss:\t 2.295988877123598\n",
      "(0, 0, 4, 15)\tLoss:\t 2.2959887230235867\n",
      "(0, 0, 5, 0)\tLoss:\t 2.2959849091793334\n",
      "(0, 0, 5, 1)\tLoss:\t 2.295983591367998\n",
      "(0, 0, 5, 2)\tLoss:\t 2.2959734755454333\n",
      "(0, 0, 5, 3)\tLoss:\t 2.2959723446170752\n",
      "(0, 0, 5, 4)\tLoss:\t 2.2959483439304282\n",
      "(0, 0, 5, 5)\tLoss:\t 2.295948320538458\n",
      "(0, 0, 5, 6)\tLoss:\t 2.295943391252521\n",
      "(0, 0, 5, 7)\tLoss:\t 2.2959422637422833\n",
      "(0, 0, 5, 8)\tLoss:\t 2.2959418754975474\n",
      "(0, 0, 5, 9)\tLoss:\t 2.295940361786351\n",
      "(0, 0, 5, 10)\tLoss:\t 2.295933831378008\n",
      "(0, 0, 5, 11)\tLoss:\t 2.2959290644910926\n",
      "(0, 0, 5, 12)\tLoss:\t 2.2959261697568376\n",
      "(0, 0, 5, 13)\tLoss:\t 2.295925786450847\n",
      "(0, 0, 5, 14)\tLoss:\t 2.2959245275753517\n",
      "(0, 0, 5, 15)\tLoss:\t 2.295918113463986\n",
      "(0, 1, 0, 0)\tLoss:\t 2.2959153014372258\n",
      "(0, 1, 0, 1)\tLoss:\t 2.2959150178128347\n",
      "(0, 1, 0, 2)\tLoss:\t 2.2958996463001173\n",
      "(0, 1, 0, 3)\tLoss:\t 2.2958996447209277\n",
      "(0, 1, 0, 4)\tLoss:\t 2.2958966683628717\n",
      "(0, 1, 0, 5)\tLoss:\t 2.2958961353649303\n",
      "(0, 1, 0, 6)\tLoss:\t 2.29589612583733\n",
      "(0, 1, 0, 7)\tLoss:\t 2.2958961227814463\n",
      "(0, 1, 0, 8)\tLoss:\t 2.2958960184948936\n",
      "(0, 1, 0, 9)\tLoss:\t 2.2958960170071805\n",
      "(0, 1, 0, 10)\tLoss:\t 2.295894049242282\n",
      "(0, 1, 0, 11)\tLoss:\t 2.2958893358541497\n",
      "(0, 1, 0, 12)\tLoss:\t 2.295889332422282\n",
      "(0, 1, 0, 13)\tLoss:\t 2.2958891420727756\n",
      "(0, 1, 0, 14)\tLoss:\t 2.2958890752179637\n",
      "(0, 1, 0, 15)\tLoss:\t 2.29588898417371\n",
      "(0, 1, 1, 0)\tLoss:\t 2.2958870373392006\n",
      "(0, 1, 1, 1)\tLoss:\t 2.2958869294335154\n",
      "(0, 1, 1, 2)\tLoss:\t 2.2958866483218294\n",
      "(0, 1, 1, 3)\tLoss:\t 2.29588622410645\n",
      "(0, 1, 1, 4)\tLoss:\t 2.2958861398900074\n",
      "(0, 1, 1, 5)\tLoss:\t 2.295886137740012\n",
      "(0, 1, 1, 6)\tLoss:\t 2.2958610556808514\n",
      "(0, 1, 1, 7)\tLoss:\t 2.2958606877866305\n",
      "(0, 1, 1, 8)\tLoss:\t 2.2958573445707486\n",
      "(0, 1, 1, 9)\tLoss:\t 2.295857258005227\n",
      "(0, 1, 1, 10)\tLoss:\t 2.295853733442625\n",
      "(0, 1, 1, 11)\tLoss:\t 2.2958535285224935\n",
      "(0, 1, 1, 12)\tLoss:\t 2.2958530473355463\n",
      "(0, 1, 1, 13)\tLoss:\t 2.2958528963721188\n",
      "(0, 1, 1, 14)\tLoss:\t 2.2958514704164834\n",
      "(0, 1, 1, 15)\tLoss:\t 2.2958505669856644\n",
      "(0, 1, 2, 0)\tLoss:\t 2.295849747467912\n",
      "(0, 1, 2, 1)\tLoss:\t 2.2958494820421684\n",
      "(0, 1, 2, 2)\tLoss:\t 2.2958464581093425\n",
      "(0, 1, 2, 3)\tLoss:\t 2.2958462354915694\n",
      "(0, 1, 2, 4)\tLoss:\t 2.29584573992246\n",
      "(0, 1, 2, 5)\tLoss:\t 2.2958456027196914\n",
      "(0, 1, 2, 6)\tLoss:\t 2.2958438781698955\n",
      "(0, 1, 2, 7)\tLoss:\t 2.2958438729613144\n",
      "(0, 1, 2, 8)\tLoss:\t 2.2958433723367504\n",
      "(0, 1, 2, 9)\tLoss:\t 2.2958421786877548\n",
      "(0, 1, 2, 10)\tLoss:\t 2.295839480193677\n",
      "(0, 1, 2, 11)\tLoss:\t 2.2958390003616653\n",
      "(0, 1, 2, 12)\tLoss:\t 2.2958389550932674\n",
      "(0, 1, 2, 13)\tLoss:\t 2.295838780244191\n",
      "(0, 1, 2, 14)\tLoss:\t 2.29583853513895\n",
      "(0, 1, 2, 15)\tLoss:\t 2.2958380310571416\n",
      "(0, 1, 3, 0)\tLoss:\t 2.295836967742724\n",
      "(0, 1, 3, 1)\tLoss:\t 2.2958354421897784\n",
      "(0, 1, 3, 2)\tLoss:\t 2.295818235266848\n",
      "(0, 1, 3, 3)\tLoss:\t 2.295815577275631\n",
      "(0, 1, 3, 4)\tLoss:\t 2.2957926530303467\n",
      "(0, 1, 3, 5)\tLoss:\t 2.2957925303160676\n",
      "(0, 1, 3, 6)\tLoss:\t 2.2957749270487624\n",
      "(0, 1, 3, 7)\tLoss:\t 2.295773684585833\n",
      "(0, 1, 3, 8)\tLoss:\t 2.2957726656033706\n",
      "(0, 1, 3, 9)\tLoss:\t 2.2957726371438762\n",
      "(0, 1, 3, 10)\tLoss:\t 2.2957720231269096\n",
      "(0, 1, 3, 11)\tLoss:\t 2.295771862075456\n",
      "(0, 1, 3, 12)\tLoss:\t 2.295769790000763\n",
      "(0, 1, 3, 13)\tLoss:\t 2.2957684440552866\n",
      "(0, 1, 3, 14)\tLoss:\t 2.295767177468182\n",
      "(0, 1, 3, 15)\tLoss:\t 2.2957669252135195\n",
      "(0, 1, 4, 0)\tLoss:\t 2.2957575249101603\n",
      "(0, 1, 4, 1)\tLoss:\t 2.295756968777337\n",
      "(0, 1, 4, 2)\tLoss:\t 2.2957434010650535\n",
      "(0, 1, 4, 3)\tLoss:\t 2.295743052906907\n",
      "(0, 1, 4, 4)\tLoss:\t 2.2957286788533047\n",
      "(0, 1, 4, 5)\tLoss:\t 2.29572858906353\n",
      "(0, 1, 4, 6)\tLoss:\t 2.295718800645064\n",
      "(0, 1, 4, 7)\tLoss:\t 2.295717659628856\n",
      "(0, 1, 4, 8)\tLoss:\t 2.29571473082816\n",
      "(0, 1, 4, 9)\tLoss:\t 2.2957145185551013\n",
      "(0, 1, 4, 10)\tLoss:\t 2.2957142185100894\n",
      "(0, 1, 4, 11)\tLoss:\t 2.2957135618621187\n",
      "(0, 1, 4, 12)\tLoss:\t 2.2957127811409355\n",
      "(0, 1, 4, 13)\tLoss:\t 2.2957122133004164\n",
      "(0, 1, 4, 14)\tLoss:\t 2.2957121724713554\n",
      "(0, 1, 4, 15)\tLoss:\t 2.295710699198658\n",
      "(0, 1, 5, 0)\tLoss:\t 2.2957059404082396\n",
      "(0, 1, 5, 1)\tLoss:\t 2.2957051555199675\n",
      "(0, 1, 5, 2)\tLoss:\t 2.295686388495671\n",
      "(0, 1, 5, 3)\tLoss:\t 2.2956832773999434\n",
      "(0, 1, 5, 4)\tLoss:\t 2.2956736128183777\n",
      "(0, 1, 5, 5)\tLoss:\t 2.2956735513059776\n",
      "(0, 1, 5, 6)\tLoss:\t 2.295673193075208\n",
      "(0, 1, 5, 7)\tLoss:\t 2.2956721275934875\n",
      "(0, 1, 5, 8)\tLoss:\t 2.2956702947510736\n",
      "(0, 1, 5, 9)\tLoss:\t 2.2956695487475254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-08e848d18e41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_operation_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-08e848d18e41>\u001b[0m in \u001b[0;36mpipe_operation_conv\u001b[0;34m(np_ar, conv_w, conv_b, activ_func)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactiv_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_ar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/caiomoraes/Acer/Users/CaioMoraes/Documents/#Computação UFPB/18.1/Top Esp em PDI/DeepProj2/deep_utils.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(np_ar, kernel, stride, bias)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_ar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mein_eq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/corning_inspect/lib/python3.6/site-packages/numpy/core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;31m# If no optimization, run pure einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptimize_arg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0mvalid_einsum_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'order'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'casting'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train():\n",
    "    batch_size = 32\n",
    "    h = 0.00000001\n",
    "    learning_rate = 1\n",
    "    batches = 3\n",
    "    # prev_dw = np.inf\n",
    "    b_idx = 1\n",
    "    loss_threshold = 0.01\n",
    "    end_training = False\n",
    "\n",
    "    for batch, classes in batch_gen(train_dataset, train_labels, batch_size=batch_size, steps=batches, normalization=True):\n",
    "\n",
    "        print('BATCH: ', b_idx)\n",
    "        b_idx += 1\n",
    "        classes = np.argmax(classes, axis=1)\n",
    "\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            print('layer:', layer_idx)\n",
    "\n",
    "            for idx, val in np.ndenumerate(layer):\n",
    "\n",
    "                # calculando L(w)\n",
    "                Lw_results = np.zeros((batch_size,))\n",
    "                for i, img, cl in zip(range(batch_size), batch, classes):\n",
    "                    softscores = softmax(forward(img))\n",
    "                    Lw_results[i] = -np.log(softscores[cl])\n",
    "\n",
    "                # calculando L(w+h)\n",
    "                layer[idx] += h\n",
    "                Lwh_results = np.zeros((batch_size,))\n",
    "                for i, img, cl in zip(range(batch_size), batch, classes):\n",
    "                    softscores = softmax(forward(img))\n",
    "                    Lwh_results[i] = -np.log(softscores[cl])\n",
    "                layer[idx] -= h\n",
    "\n",
    "                Lw =  Lw_results.mean()\n",
    "                Lwh = Lwh_results.mean()\n",
    "\n",
    "                # dw\n",
    "                dw = (Lwh - Lw) / h\n",
    "\n",
    "                #========================================#\n",
    "                # SGD\n",
    "                print(idx, end='\\t')\n",
    "                print('Loss:\\t', Lw)\n",
    "                layer[idx] -= dw * learning_rate\n",
    "\n",
    "                if Lwh < loss_threshold:\n",
    "                    return\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
